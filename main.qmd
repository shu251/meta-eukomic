---
title: "Meta-eukomic"
format:
  html:
    toc: true
    toc-location: left
    number-sections: true
    number-depth: 2
editor: visual
---

# Meta-eukomic

*SKH* last updated June 2024

Location for raw files: `/scratch/group/hu-lab/meta-eukomics/raw-data` Individual R1 and R2 files are 2.9 and 2.7 G, respectively.

## Background reading

1.  Cohen NR, Alexander H, Krinos AI, Hu SK, Lampe RH. Marine Microeukaryote Metatranscriptomics: Sample Processing and Bioinformatic Workflow Recommendations for Ecological Applications. Frontiers in Marine Science 2022; 9.

2.  Krinos AI, Cohen NR, Follows MJ, Alexander H. Reverse engineering environmental metatranscriptomes clarifies best practices for eukaryotic assembly. BMC Bioinformatics 2023; 24: 74.

## Working environment

High performace computer (HPC) hosted by my University. This is the TAMU HPRC

# Step-by-step

## 1. Determine assembly groups

## 2. Trim & QC

*Initial Fastqc*

Code for slurm script run on the HPC to run fastqc (`fastqc.slurm`). For this set of samples, it took 10 minutes and used 290 MB. CPU used: 00:12:27.

```{r}       
module load FastQC/0.11.9-Java-11

fastqc /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_001.fastq.gz
fastqc /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_001.fastq.gz
```


In order to look at the output `.html` files, they need to be opened locally.

```{r}  
scp $HPRC-ADDRESS:/scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_001_fastqc.html meta-eukomic/output-files/
```

Then we need to trim the individual reads, removing any sequencing-based primers, etc. We can use the program, [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) for this. R1 reads are forward and R2 are reverse. The Trimmomatic software requires input reads and then output trimmed and unpaired reads (the latter of which are discarded). Another input file required is a list of the possible primers and adapters from sequencing, `adapters-primers.fa`.

Slurm script, `trim_fastqc.slurm`. Below, trim parameter include:

* Remove adapters, found in `adapters-primers.fa`

* Remove leading low quality or N bases (below quality 3) (LEADING:3)

* Remove trailing low quality or N bases (below quality 3) (TRAILING:3)

* Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 10 (SLIDINGWINDOW:4:10)

* Drop reads shorter than 50 bases long (MINLEN:50)

```{r}           
module load Trimmomatic/0.39-Java-11

java -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar PE /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_001.fastq.gz /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_001.fastq.gz  /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_trimmed.fastq.gz /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_unpaired.fastq.gz /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_trimmed.fastq.gz /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_unpaired.fastq.gz ILLUMINACLIP:adapters-primers.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:10 MINLEN:50

# "$EBROOTTRIMMOMATIC/trimmomatic-0.39.jar" note that this is specific to the HPRC system we are using

echo "Trimmomtatic complete. Repeating fastqc"

module load FastQC/0.11.9-Java-11

fastqc /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_trimmed.fastq.gz
fastqc /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_trimmed.fastq.gz
```

## 3. Remove ribosomal RNA

[Sortmerna](https://sortmerna.readthedocs.io/en/latest/installation.html) installation guidelines and usage can be found here.

On the TAMU HPRC, this is where we needed to make a conda environment to run this.

```{r}  
# Load anaconda
module load Anaconda3/2022.05

# modify bioconda config for sortmeRNA
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --set channel_priority strict
```

Search for sortmerna using conda:
```{r}  
conda search sortmerna
```
Create a conda environment specific for sortmerna, activate it, and install it.
```{r}  
conda create --name sortmerna_env
conda activate sortmerna_env
conda install sortmerna
```

Answer "yes" (*Y*) to installation questions. Now when you run SortMeRNA, you need to activate this conda environment and execute your code. The next step is to download the databases to use for this program.


#### SortMeRNA databases

Separately, you need to download the databases to "align" and check the sequences for ribosomal RNAs.


Instructions for [downloading version 4](https://sortmerna.readthedocs.io/en/latest/databases.html):
```{r}  
# Move to the location you want your rRNA databases stored
cd /scratch/group/hu-lab/meta-eukomics/ 

# Download from github release 
wget https://github.com/biocore/sortmerna/releases/download/v4.3.4/database.tar.gz

# Make new directory for this and un-zip downloaded file
mkdir rRNA_databases_v4
tar -xvf database.tar.gz -C rRNA_databases_v4
```

### Run SortMeRNA


In your slurm script, activate conda, and then the sortmerna environment:
```{r}  
# Load anaconda
module load Anaconda3/2022.05
conda activate sortmerna_env

mkdir /scratch/group/hu-lab/meta-eukomics/rRNA-sort

sortmerna -ref /scratch/group/hu-lab/meta-eukomics/rRNA_databases_v4/smr_v4.3_sensitive_db.fasta \
-reads /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_trimmed.fastq.gz \
-reads /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_trimmed.fastq.gz \
-workdir /scratch/group/hu-lab/meta-eukomics/rRNA-sort/
-fastx --paired_in --out2 --threads $SLURM_CPUS_PER_TASK
```

`paired_in`: The option ‘paired_in’ is optimal for users who want all reads in the other.fasta file to be non-rRNA. However, there are small chances that reads which are non-rRNA will also be put into the aligned.fasta file.

Since the SortmeRNA tool is optimitized to take things that are aligned with the rRNA databases and put them in a file called "aligned", then we need to use the `other.fasta` output file. 

How do I properly run sortmerna? https://github.com/sortmerna/sortmerna/issues/333



## 4. Assembly

The next step is to take all the sequenced reads and bring them together to make longer, more continuous sequences, called *contigs*. Here, we will use two assemblers and combine the results. Each one is built slightly differently.

### MEGAHIT (assembly 1)

We will first use [megahit](https://github.com/voutcn/megahit). To save the assemblies separately, make a new assembly output file in your working `scratch` space. `mkdir /scratch/group/hu-lab/meta-eukomics/assembly`

The megahit command below, outputs contigs in the new `assembly` directory, only keeps reads longer than 100 bps, and uses the megahit preset for lots of diversity in a sample (_meta-large_).

Megahit uses multiple k-mer strategy, and we can set the min and max k. In order to reduce the complexity of the _de Bruijin_ graph, a kmin size of 25 and higher (like 27) is better. 


See slurm script: `megahit-assembly.slurm`
```{r}           
module load GCCcore/11.2.0
module load MEGAHIT/1.2.9

megahit -1 /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_trimmed.fastq.gz -2 /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_trimmed.fastq.gz -o /scratch/group/hu-lab/meta-eukomics/assembly/ --min-contig-len 1000 --presets meta-large --num-cpu-threads 32
```

With the above settings, megahit recovered 31,331 contigs, total 44610184 bp, min 1000 bp, max 10674 bp, avg 1423 bp, N50 1391 bp. And this took about *10 hours*.

On the HPC, this was run on 1 node, 4 cores per node, and it used 18.5 GBs of memory.


Megahit assembly location: `/scratch/group/hu-lab/meta-eukomics/assembly/megahit-output/final.contigs.fa`


*_Optional_: Visualize your megahit assembly*

When megahit assembly is complete, use this option to visualize it: https://github.com/voutcn/megahit/wiki/Visualizing-MEGAHIT's-contig-graph


### IDBA-Tran

[IDBA-tran](https://i.cs.hku.hk/~alse/hkubrg/projects/idba_tran/index.html) is another _de novo_ assembler. Uses local assembly to reconstruct missing kmers in low-expressed transcripts.

See script: `idba-assembly.slurm` to run assembly with minimum kmer at 20 and max kmer at 50 with a 5 step increment of kmer.
```{r}  
echo "unzip files"
gunzip /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R*_trimmed.fastq.gz

module load GCC/8.2.0-2.31.1 
module load IDBA-UD/1.1.3

# Convert fastq R1 and R2 files into a single 'interleaved` fastq files
fq2fa --merge --filter /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_trimmed.fastq /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_trimmed.fastq /scratch/group/hu-lab/meta-eukomics/assembly/merged-PE-for-idba.fa

# Run assembly
idba_tran -r /scratch/group/hu-lab/meta-eukomics/assembly/merged-PE-for-idba.fa -o /scratch/group/hu-lab/meta-eukomics/assembly/idba-out --mink 20 --maxk 50 --step 5 --num_threads 16

# Make sure fastq files are re-zipped:
gzip /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_trimmed.fastq
gzip /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_trimmed.fastq
```

Using 8 cores, the IDBA-tran assembly took about 23 hours and used 130 GB of memory.


The *IDBA-tran output* includes separation by k-mer length. So there are multiple "contig-X.fa" files. Where `contig-25.fa` is the output contigs at k-mer = 25. The IDBA program determines which one is best and then puts this into the final `contig.fa` file. 
Output: `/scratch/group/hu-lab/meta-eukomics/assembly/idba-out/contig.fa`

## 5. Evaluate assemblies

### QUAST

QUAST stands for _QUality ASsessment Tool_ and it is a tool for evaluating assemblies. Not all tools for evaluating assemblies will work, as most are built for metagenomics or require a reference genome. In this case, we do not have a reference genome. 


*First for the IDBA output*:

```{r}  
module load GCC/9.3.0
module load OpenMPI/4.0.3
module load QUAST/5.0.2-Python-3.8.2

quast.py /scratch/group/hu-lab/meta-eukomics/assembly/idba-out/contig.fa \
        -1 /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_trimmed.fastq.gz -2 /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_trimmed.fastq.gz \
        -o /scratch/group/hu-lab/meta-eukomics/assembly/idba-out/ \
        --threads $SLURM_CPUS_PER_TASK
```

This was run on 1 node with 16 cores. It used 3.5 GB of memory in about 1 hour. 6:15 CPUs used.



*Repeat for the megahit output*

```{r}  
module load GCC/9.3.0
module load OpenMPI/4.0.3
module load QUAST/5.0.2-Python-3.8.2

quast.py /scratch/group/hu-lab/meta-eukomics/assembly/megahit-output/final.contigs.fa \
        -1 /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R1_trimmed.fastq.gz -2 /scratch/group/hu-lab/meta-eukomics/raw-data/HS039_S90_L004_R2_trimmed.fastq.gz \
        -o /scratch/group/hu-lab/meta-eukomics/assembly/megahit-output/ \
        --threads $SLURM_CPUS_PER_TASK
```


#### Interpret QUAST 

Download files locally from:

  * PDF files: `meta-eukomics/assembly/*-out/basic_stats/*.pdf `
  * Use an ICARUS html viewer: `meta-eukomics/assembly/*-out/icarus* `
  * Basic read stats: `meta-eukomics/assembly/*-out/reads_stats/reads_report.tsv`


## 6. Cluster assembly output (mmseqs2)

Because we have outputs from two separate assemblies, we can use a program to "cluster" the output contigs together. The clustering / merging of contigs is based on sequence similarity, so we need to include a similarity parameter that is kind of arbitrary.


We will use [mmseqs2](https://github.com/soedinglab/MMseqs2) and try this at 99% similarity. MMseq stands for "Many-against-Many sequence searching"


First, the output contig files from IDBA and Megahit are combined.


```{r}  
# Precursor load to use MMseqs2 on HPRC
module load GCC/10.2.0
module load OpenMPI/4.0.5
module load MMseqs2/13-45111

mkdir /scratch/group/hu-lab/meta-eukomics/assembly/combined-assembly

cat /scratch/group/hu-lab/meta-eukomics/assembly/megahit-output/final.contigs.fa /scratch/group/hu-lab/meta-eukomics/assembly/idba-out/contig.fa > /scratch/group/hu-lab/meta-eukomics/assembly/combined-assembly/combined-assemblies.fasta


# Use the 'easy-linclust' option

mmseqs easy-linclust /scratch/group/hu-lab/meta-eukomics/assembly/combined-assembly/combined-assemblies.fasta /scratch/group/hu-lab/meta-eukomics/assembly/combined-assembly/clusterRes /scratch/group/hu-lab/meta-eukomics/assembly/combined-assembly/tmp --threads=$SLURM_CPUS_PER_TASK

```


## 7. Transdecoder

```{r}
module load GCC/11.3.0
module load TransDecoder/5.5.0

#extract the long open reading frames
# TransDecoder.LongOrfs -t target_transcripts.fasta

# predict the likely coding regions
# TransDecoder.Predict -t target_transcripts.fasta [ homology options ]
```



### 7.1 QUAST new combined assembly

## 8. Marferret 



## Metatranscriptome run information

| Software    | Version |
|-------------|---------|
| Fastqc      | 0.11.9  |
| Trimmomatic | 0.39    |
| SortMeRNA   | 4.3.7   |
| megahit     | 1.2.9   |
| IDBA-Tran   | 1.1.3   |
| quast       | 5.0.2   |
| mmseqs2     |         |
| TransDecoder|5.5.0    |
|             |         |
|             |         |
|             |         |

#### Citations

* [fastqc]

* [trimmomatic]

* SortMeRNA: Kopylova E., Noe L. and Touzet H., “SortMeRNA: Fast and accurate filtering of ribosomal RNAs in metatranscriptomic data”, Bioinformatics (2012), doi: 10.1093/bioinformatics/bts611.

* [megahit]

* [IDBA-Tran](https://academic.oup.com/bioinformatics/article/29/13/i326/191893)


# eukrhythmic

Guidelines and notes on running the same set of samples through the [eukrhythmic pipeline](https://eukrhythmic.readthedocs.io/en/latest/index.html). Running this on the TAMU HPRC with a conda environment.

First create a conda environment specific for eukrhythmic. [Learn more about how to manage these environments here.](https://docs.conda.io/projects/conda/en/4.6.1/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands)

```
conda create -n eukrhythmic

# Activate environment
conda activate eukrhythmic

# Use HPRC system to ensure newest version of available Python is loaded
> (eukrhythmic) module load GCCcore/12.3.0
> (eukrhythmic) module load Python/3.11.3
```
To check what version of python you are automatically running, use `python --version`.

* I could not get mamba configured correctly?
```
> (eukrhythmic) conda install conda-forge::pyyaml
> (eukrhythmic) conda install -c conda-forge python pandas

# Install snakemake, but use mamba
> (eukrhythmic) conda install -c conda-forge mamba

mamba install -c conda-forge -c bioconda snakemake
```

To run eukrhythmic, activate the environment: `conda activate eukrhythmic`.


```{r}
```

